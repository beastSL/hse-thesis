{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "from crowdkit.aggregation import Wawa\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mv_or_median(x):\n",
    "    vc = x.value_counts()\n",
    "    if vc.values[0] == 1:\n",
    "        return int(np.median(x))\n",
    "    else:\n",
    "        return int(vc.index[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wawa_score(toloka_markup):\n",
    "    for_wawa = toloka_markup[['OUTPUT:is_subjective', 'ASSIGNMENT:task_id', 'ASSIGNMENT:worker_id']].rename({\n",
    "        'OUTPUT:is_subjective': 'label',\n",
    "        'ASSIGNMENT:task_id': 'task',\n",
    "        'ASSIGNMENT:worker_id': 'worker'\n",
    "    }, axis=1)\n",
    "    return Wawa().fit_predict(for_wawa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_unaligned_workers(scores_path, threshold=0.35):\n",
    "    toloka_markup = pd.read_csv(scores_path, sep='\\t').drop([\"GOLDEN:is_subjective\", \"HINT:text\", \"HINT:default_language\"], axis=1)\n",
    "    toloka_markup['OUTPUT:is_subjective'].replace({2: 1, 3: 2, 4: 3, 5: 3}, inplace=True)\n",
    "\n",
    "    wawa_score = get_wawa_score(toloka_markup)\n",
    "\n",
    "    worker_ids = toloka_markup.value_counts(\"ASSIGNMENT:worker_id\").index\n",
    "    worker_f1s = []\n",
    "    for worker_id in worker_ids:\n",
    "        worker_scores = toloka_markup[toloka_markup['ASSIGNMENT:worker_id'] == worker_id][['ASSIGNMENT:task_id', 'OUTPUT:is_subjective']]\n",
    "        worker_scores = worker_scores.set_index('ASSIGNMENT:task_id')\n",
    "        worker_scores['wawa'] = wawa_score[wawa_score.index.isin(worker_scores.index)]\n",
    "        worker_f1s.append(f1_score(worker_scores['wawa'], worker_scores['OUTPUT:is_subjective'], average='macro'))\n",
    "    wawa_worker_ratings = pd.Series(data=worker_f1s, index=worker_ids)\n",
    "    return wawa_worker_ratings[wawa_worker_ratings < 0.35]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_label_distribution(scores_path):\n",
    "    toloka_markup = pd.read_csv(scores_path, sep='\\t').drop([\"GOLDEN:is_subjective\", \"HINT:text\", \"HINT:default_language\"], axis=1)\n",
    "    toloka_markup['OUTPUT:is_subjective'].replace({2: 1, 3: 2, 4: 3, 5: 3}, inplace=True)\n",
    "\n",
    "    wawa_score = get_wawa_score(toloka_markup)\n",
    "\n",
    "    vc = wawa_score.value_counts()\n",
    "    print(f\"Sentences with the 'Non-Applicable' label: {vc[0]}\")\n",
    "    print(f\"Sentences with the 'Objective' label: {vc[1]}\")\n",
    "    print(f\"Sentences with the 'Neutral' label: {vc[2]}\")\n",
    "    print(f\"Sentences with the 'Subjective' label: {vc[3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score_by_sentence_id(scores_path, sentence_id_retrieval_path):\n",
    "    toloka_markup = pd.read_csv(scores_path, sep='\\t').drop([\"GOLDEN:is_subjective\", \"HINT:text\", \"HINT:default_language\"], axis=1)\n",
    "    toloka_markup['OUTPUT:is_subjective'].replace({2: 1, 3: 2, 4: 3, 5: 3}, inplace=True)\n",
    "\n",
    "    sentence_ids_by_text = pd.read_csv(sentence_id_retrieval_path, sep='\\t').set_index(\"INPUT:text\")\n",
    "\n",
    "    wawa_score = get_wawa_score(toloka_markup)\n",
    "    \n",
    "    wawa_score_df = pd.DataFrame(wawa_score).rename(columns={\"agg_label\": \"score\"})\n",
    "    wawa_score_df = wawa_score_df.join(toloka_markup[[\"INPUT:text\", \"ASSIGNMENT:task_id\"]].drop_duplicates().set_index(\"ASSIGNMENT:task_id\"))\n",
    "    wawa_score_df[\"INPUT:text\"] = wawa_score_df[\"INPUT:text\"].str.replace(\"\\r\", \"\")\n",
    "    wawa_score_df = wawa_score_df.set_index(\"INPUT:text\").join(sentence_ids_by_text).reset_index().drop(\"INPUT:text\", axis=1)\n",
    "    wawa_score_df = wawa_score_df.groupby(by=\"sentence_id\")['score'].apply(get_mv_or_median).to_frame()\n",
    "\n",
    "    assert wawa_score_df.isna().value_counts()[False] == len(wawa_score_df)\n",
    "\n",
    "    return wawa_score_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_with_data_and_split(aggregated_scores, data_path):\n",
    "    data = pd.read_csv(data_path).join(aggregated_scores, on=\"sentence_id\").rename(columns={\"score\":\"is_subjective\"})\n",
    "    train_val, test = train_test_split(data, test_size=0.2, stratify=data['is_subjective'])\n",
    "    train, val = train_test_split(train_val, test_size=0.1, stratify=train_val['is_subjective'])\n",
    "    return train, val, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], dtype: float64)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_unaligned_workers(\"testing_scores.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], dtype: float64)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_unaligned_workers(\"full_scores.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences with the 'Non-Applicable' label: 15\n",
      "Sentences with the 'Objective' label: 52\n",
      "Sentences with the 'Neutral' label: 1\n",
      "Sentences with the 'Subjective' label: 89\n"
     ]
    }
   ],
   "source": [
    "print_label_distribution(\"testing_scores.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences with the 'Non-Applicable' label: 1236\n",
      "Sentences with the 'Objective' label: 14222\n",
      "Sentences with the 'Neutral' label: 388\n",
      "Sentences with the 'Subjective' label: 3861\n"
     ]
    }
   ],
   "source": [
    "print_label_distribution(\"full_scores.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentence_id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19948.0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19949.0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19950.0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19951.0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19952.0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19953 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             score\n",
       "sentence_id       \n",
       "0.0              1\n",
       "1.0              1\n",
       "2.0              1\n",
       "3.0              1\n",
       "4.0              1\n",
       "...            ...\n",
       "19948.0          1\n",
       "19949.0          1\n",
       "19950.0          1\n",
       "19951.0          1\n",
       "19952.0          1\n",
       "\n",
       "[19953 rows x 1 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_scores = pd.read_csv(\"sample_scores.csv\").set_index(\"sentence_id\").replace({2: 1, 3: 2, 4: 3, 5: 3})\n",
    "testing_aggregated_scores = get_score_by_sentence_id(\"testing_scores.tsv\", \"testing_dataset_for_retrieval.tsv\")\n",
    "full_aggregated_scores = get_score_by_sentence_id(\"full_scores.tsv\", \"full_dataset_for_retrieval.tsv\")\n",
    "\n",
    "aggregated_scores = pd.concat([sample_scores, testing_aggregated_scores, full_aggregated_scores])\n",
    "aggregated_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val, test = join_with_data_and_split(aggregated_scores, \"sent_tokenized_dataset.csv\")\n",
    "\n",
    "train.to_csv(\"train.csv\", index=False)\n",
    "val.to_csv(\"val.csv\", index=False)\n",
    "test.to_csv(\"test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    10263\n",
       "3     2819\n",
       "0      911\n",
       "2      372\n",
       "Name: is_subjective, dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(\"train.csv\")\n",
    "train['is_subjective'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
