{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pymorphy2\n",
    "import re\n",
    "import functools\n",
    "import ssl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/beast-sl/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/beast-\n",
      "[nltk_data]     sl/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/beast-\n",
      "[nltk_data]     sl/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/beast-\n",
      "[nltk_data]     sl/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "en_nlp = spacy.load('en_core_web_sm')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../datasets/nsdc_without_neutrality/train.csv')\n",
    "val = pd.read_csv('../datasets/nsdc_without_neutrality/val.csv')\n",
    "test = pd.read_csv('../datasets/nsdc_without_neutrality/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constant baseline\n",
      "Val: F1 0.28365960846983623, Accuracy 0.7406119610570236\n",
      "Test: F1 0.28358367955683395, Accuracy 0.7402669632925473\n"
     ]
    }
   ],
   "source": [
    "print(\"Constant baseline\")\n",
    "val_preds = [1] * len(val)\n",
    "print(f\"Val: F1 {f1_score(val['score'], val_preds, average='macro')}, Accuracy {accuracy_score(val['score'], val_preds)}\")\n",
    "test_preds = [1] * len(test)\n",
    "print(f\"Test: F1 {f1_score(test['score'], test_preds, average='macro')}, Accuracy {accuracy_score(test['score'], test_preds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(texts):\n",
    "    m = pymorphy2.MorphAnalyzer()\n",
    "    mystopwords = stopwords.words('english') \n",
    "\n",
    "    def tokenize(text):\n",
    "        regex = re.compile(\"[A-Za-z]+\")\n",
    "        try:\n",
    "            return regex.findall(text.lower())\n",
    "        except:\n",
    "            return []\n",
    "\n",
    "    @functools.lru_cache(maxsize=128)\n",
    "    def lemmatize_word(token, pymorphy=m):\n",
    "        return pymorphy.parse(token)[0].normal_form\n",
    "\n",
    "    def lemmatize_text(text):\n",
    "        return [lemmatize_word(w) for w in text]\n",
    "\n",
    "    def remove_stopwords(lemmas, stopwords = mystopwords):\n",
    "        return [w for w in lemmas if not w in stopwords]\n",
    "\n",
    "    preprocessed_texts = []\n",
    "    corpus = texts\n",
    "    for sample in corpus:\n",
    "        text = \"\"\n",
    "        label = 0\n",
    "        text = sample\n",
    "        preprocessed_text = remove_stopwords(lemmatize_text(tokenize(text)))\n",
    "        preprocessed_texts.append(preprocessed_text)\n",
    "    return np.array(preprocessed_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6z/lsnxrtxs6glb8kdyjtmj4ypw0000gn/T/ipykernel_19960/3096039493.py:30: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.array(preprocessed_texts)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF + LogReg baseline\n",
      "Val: F1 0.4838583553503856, Accuracy 0.7739916550764951\n",
      "Test: F1 0.42707021296056796, Accuracy 0.7627919911012235\n"
     ]
    }
   ],
   "source": [
    "X_train = preprocess(list(train['text']))\n",
    "y_train = train['score']\n",
    "X_val = preprocess(list(val['text']))\n",
    "y_val = train['score']\n",
    "X_test = preprocess(list(test['text']))\n",
    "y_test = train['score']\n",
    "vectorizer = TfidfVectorizer(preprocessor=lambda x: x, tokenizer=lambda x: x)\n",
    "train_tf_idf = vectorizer.fit_transform(X_train)\n",
    "val_tf_idf = vectorizer.transform(X_val)\n",
    "test_tf_idf = vectorizer.transform(X_test)\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(train_tf_idf, y_train)\n",
    "val_preds = model.predict(val_tf_idf)\n",
    "test_preds = model.predict(test_tf_idf)\n",
    "print(\"TF-IDF + LogReg baseline\")\n",
    "print(f\"Val: F1 {f1_score(val['score'], val_preds, average='macro')}, Accuracy {accuracy_score(val['score'], val_preds)}\")\n",
    "print(f\"Test: F1 {f1_score(test['score'], test_preds, average='macro')}, Accuracy {accuracy_score(test['score'], test_preds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
