\documentclass[a4paper,14pt]{extarticle}
\usepackage{geometry}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english,russian]{babel}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{fancyhdr}
\usepackage{setspace}
\usepackage{graphicx}
\usepackage{colortbl}
\usepackage{tikz}
\usepackage{pgf}
\usepackage{subcaption}
\usepackage{listings}
\usepackage{indentfirst}
\usepackage[colorlinks,citecolor=blue,linkcolor=blue,bookmarks=false,hypertexnames=true, urlcolor=blue]{hyperref} 
\usepackage{indentfirst}
\usepackage{mathtools}
\usepackage{booktabs}
\usepackage[flushleft]{threeparttable}
\usepackage{tablefootnote}
\usepackage{makecell}
\usepackage{microtype}


\usepackage{chngcntr} % нумерация графиков и таблиц по секциям
\counterwithin{table}{section}
\counterwithin{figure}{section}

\graphicspath{{graphics/}}%путь к рисункам

\makeatletter
\renewcommand{\@biblabel}[1]{#1.} % Заменяем библиографию с квадратных скобок на точку:
\makeatother

\geometry{left=2.5cm}% левое поле
\geometry{right=1.5cm}% правое поле
\geometry{top=1.5cm}% верхнее поле
\geometry{bottom=1.5cm}% нижнее поле
\renewcommand{\baselinestretch}{1.5} % междустрочный интервал
\renewcommand{\theadfont}{\normalsize\bfseries}


\newcommand{\bibref}[3]{\hyperlink{#1}{#2 (#3)}} % biblabel, authors, year
\addto\captionsrussian{\def\refname{Список литературы (или источников)}} 

\renewcommand{\theenumi}{\arabic{enumi}}% Меняем везде перечисления на цифра.цифра
\renewcommand{\labelenumi}{\arabic{enumi}}% Меняем везде перечисления на цифра.цифра
\renewcommand{\theenumii}{.\arabic{enumii}}% Меняем везде перечисления на цифра.цифра
\renewcommand{\labelenumii}{\arabic{enumi}.\arabic{enumii}.}% Меняем везде перечисления на цифра.цифра
\renewcommand{\theenumiii}{.\arabic{enumiii}}% Меняем везде перечисления на цифра.цифра
\renewcommand{\labelenumiii}{\arabic{enumi}.\arabic{enumii}.\arabic{enumiii}.}% Меняем везде перечисления на цифра.цифра

\begin{document}
\input{title.tex}% это титульный лист
\newpage
\setcounter{page}{2}

{
	\hypersetup{linkcolor=black}
	\tableofcontents
}

\newpage
\begin{abstract}
    С расширением обсуждаемых тем, освещаемых в социальных сетях, читателям становится все труднее извлекать фактическую информацию из новостных статей, содержащих большое количество мнений. Эта задача также широко известна как задача обнаружения субъективности —-- отличить субъективные предложения от беспристрастных, или объективных. В этой статье представлено создание инструмента для выявления субъективности в новостных статьях с нуля. Во-первых, мы собираем современный набор данных новостных статей, аннотированных для субъективности работниками краудсорсинга. Затем мы настраиваем модель BERT, достигая точности, близкой к лучшим достигнутым результатам, в задаче обнаружения субъективности. Мы также используем продвинутые методы дообучения, такие как политика одного цикла и обучение с несколькими задачами. Наконец, мы создаем удобный инструмент для командной строки для автоматического аннотирования новостных статей с применением дообученной модели. Польза от этой работы выходит за рамки работы с новостями, поскольку точное распознавание мнений и фактов имеет важное значение для таких областей, как поиск информации и анализ тональности.
    
    Ключевые слова --- обработка естественного языка, обнаружение субъективности, краудсорсинг, сбор аннотированных данных, обработка новостей

    With the expansion of topics covered on social media, it is becoming increasingly difficult for readers to extract factual information from news articles containing a large amount of opinions. This task is also commonly known as the subjectivity detection task --- distinguishing subjective sentences from unbiased or objective sentences. This article presents the creation of a tool for detecting subjectivity in news articles from scratch. First, we collect a modern dataset of news articles annotated for subjectivity by crowdsourcing workers. We then tune the BERT model to achieve close-to-best-achieved accuracy in the subjectivity detection problem. We also use advanced learning methods such as the one cycle policy and multi-task learning. Finally, we are building a handy command-line tool for automatic annotation of news articles using a pre-trained model. The benefits of this work go beyond news analysis, as accurate recognition of opinions and facts is essential for areas such as information retrieval and sentiment analysis.

    Keywords --- natural language processing, subjectivity detection, crowdsourcing, annotated data collection, news processing
    \end{abstract}
    
    \section{Введение}
    В данной статье рассматривается задача отличия фактов от мнений в новостных статьях. Эта задача также известна как задача обнаружения субъективности. Это важная подзадача анализа тональности, поскольку модели анализа тональности работают лучше, когда работают с корпусами, содержащими исключительно оценочные суждения.
    
    В настоящее время многие действующие информационные агентства, в том числе BBC, The Guardian и другие, ежедневно публикуют тонны материалов. Количество горячо обсуждаемых тем сегодня также растет: среди недавних — экологические проблемы, COVID-19, гонка поисковых систем между Microsoft и Google и другие. С таким количеством обсуждаемых новостей обычному читателю становится трудно извлечь факты, чтобы иметь непредвзятый взгляд на происходящие события.
    
    В этой статье рассматривается создание инструмента для различения фактических и субъективных предложений в новостных статьях. Инструмент позволяет пользователю загружать текст новостной статьи и аннотировать каждое предложение меткой факта или мнения.
    
    Мы достигаем поставленной цели пошагово. Сначала мы создаём набор предложений из современных новостных статей. Этот набор данных аннотируется людьми на субъективность. Wiebe, J. et al. создали аналогичный набор данных под названием MPQA в \cite{mpqa}. Их исследование является основным аналогом нашей работы по созданию современного набора новостных данных. Аннотации предложений для этого набора данных были созданы в 1999 году в \cite{mpqa-opinion} и сейчас являются устаревшими. Во-вторых, мы дообучаем модель BERT для обнаружения субъективности. Pang, B. et al. рассмотрели различные методы дообучения модели BERT в \cite{bert-finetune}. Мы переиспользуем и проверяем результаты их исследования --- они достигли точности около 84\%, что верифицируется в данной статье. В-третьих, мы создаём новый инструмент, который по ссылке на новостную статью извлекает ее текст, размечает его на предмет субъективности и выдаёт размеченную статью. В настоящее время у данного инструмента нет аналогов; ближайший похожий инструмент — Grammarly, который может помочь обнаружить слишком оценочные предложения и адаптировать текст под настроенные критерии.
    
    Структура работы выглядит следующим образом. Раздел 2 охватывает научные статьи, связанные с нашим исследованием, то есть работы, охватывающие создание как наборов данных, так и моделей. В разделе 3 мы описываем нашу методологию для сбора и человеческой разметки набора данных. В разделе 4 мы подробно описываем дообучение модели BERT. В разделе 5 описывается создание CLI-инструмента для использования дообученной модели. В разделе 6 подробно описываются полученные результаты. Раздел 7 содержит выводы статьи.
    
    \section{Анализ литературы}
    Достаточное количество исследований было проведено в области обнаружения субъективности. Существует несколько различных наборов данных, предназначенных для этой задачи в нескольких областях. В \cite{subj} был представлен набор данных SUBJ. Он состоит из 5000 субъективных и 5000 объективных предложений, взятых с веб-сайтов IMDb и RottenTomatoes, аннотированных с помощью алгоритма. В \cite{wikipedia-biased-statements} создается набор данных, содержащий утверждения из Википедии с тегами POV (point of view). Утверждения с тегом POV — это утверждения, которые, как сообщается в статье, нарушают принцип NPOV (neutral point of view) Википедии. Эти утверждения затем аннотируются людьми, и в конечном итоге набор данных состоит из 1843 предвзятых утверждений и 3109 нейтральных. В \cite{mpqa} был представлен набор данных MPQA (Multi-Perspective Question Answering). Он содержит предложения из 535 испанских новостных статей, переведенных на английский язык и размеченных авторами на субъективность. Всего в наборе данных около 9700 предложений, и около 55\% из них субъективны. Все эти наборы данных имеют отношение к нашей задаче, но только MPQA удовлетворяет нашей области, то есть новостным статьям. Однако набор данных MPQA был создан около 20 лет назад. Мы считаем, что за 20 лет стиль новостных текстов в информационных агентств изменился; кроме того, были введены новые темы, такие как COVID-19. Было создано несколько новых агентств, а некоторые старые обанкротились. По всем этим причинам мы решили создать более современный набор данных, который в целом будет похож на MPQA, но будет состоять из современных данных.
    
    Большой объём работ был также проделан на тему аннотации текстов. В процессе разработки набора данных MPQA авторами Wiebe, J., et al. были выпущены статьи \cite{instructions-for-annotating-opinions}, \cite{annotating-expressions-of-opinions} и \cite{annotating-private-states}. Во многом они использовались как вдохновители для нашей инструкции по разметке данных, в том числе, некоторые примеры были взяты из этих статей. При создании набора данных SUBJ, в статье \cite{subj}, за изначальные данные были взяты отзывы с сайтов RottenTomatoes (отмечены как полностью субъективные) и IMDb (отмечены как полностью объективные), и взяты близости предложений из неразмеченного набора данных с размеченными. Затем был использован алгоритм min-cut-max-flow, использующий потоки, для разметки оставшейся части набора данных. В статье \cite{wikipedia-biased-statements}, самой новой из тех, что представляют новый набор данных, был использован краудсорсинг (сервис Amazon Mechanical Turk) --- аннотаторам предлагались предложения и достаточно простая инструкция; необходимо было выбрать один из трёх вариантов ответов. В настоящее время краудсорсинг стал наиболее популярной техникой разметки наборов данных, что подтверждается тем, что статья \cite{wikipedia-biased-statements} первая из нами рассмотренных стала использовать эту технику.
    
    Помимо создания наборов данных, большой объем работы был посвящен обучению моделей для выявления субъективности. В \cite{survey} показан временной прогресс таких моделей. Первые методы использовали примитивные функции, например, обнаружение ключевых слов. Затем исследователи перешли к онтологическим моделям, то есть к определению набора онтологий, которые определяют отношения между разными классами слов и проецируют их в векторное пространство. Следующими были статистические методы, похожие на классическое машинное обучение — модели, обученные на аннотированном наборе данных. Одним из примеров такой модели является Passive-Aggressive Classifier, упомянутый в \cite{fact-opinion-classifier}. Авторы этого исследования утверждают, что их метод достиг около 85\% F1-score на кросс-валидации. Это показывает, что ранние подходы к задаче были уже достаточно успешными. Следующими моделями были так называемые LDM (Latent Dirichlet Models), которые использовали частоту слов для вычисления апостериорного распределения. Более новые методы используют глубинное обучение для обнаружения субъективности. Например, существуют сверточные модели (CNN) для обнаружения субъективности, где CNN служат основной моделью, а затем ответ извлекается с помощью RNN или других методов. Один из таких методов описан в \cite{bnelm} --- они используют байесовскую сетевую машину экстремального обучения (BNELM) поверх CNN для достижения точности 89\% на TASS 2015 --- наборе данных, содержащем твиты на испанском языке для обнаружения субъективности. Наконец, новейшими моделями обнаружения субъективности являются модели, основанные на архитектуре трансформеров, такие как BERT. Одним из таких исследований является \cite{bert-finetune}, где авторы достигают точности от 84\% до 95\% на разных наборах данных. Они также пробуют различные методы дообучения для повышения качества и обнаруживают, что такие методы, как политика одного цикла и обучение с несколькими задачами являются наиболее полезными. Мы собираемся повторить этот эксперимент в нашей работе.
    
    Как упоминалось ранее, в настоящее время нет аналогов нашей работы именно для обнаружения субъективности. Существуют средства для написания новостей, такие как Headline Analyzer, Ahrefs и Grammarly, но нет инструментов для автоматического анализа новостей.
    
    \section{Сбор и разметка данных}
    \subsection{Выбор исходного набора данных}
    Перед разметкой необходимо подобрать неразмеченный набор данных. Нашими главными критериями для подбора такого набора данных были: 
    \begin{itemize}
        \item Достаточный размер: одно из главных требований к нашему размеченному набору данных было наличие достаточного количества субъективных и объективных предложений для эффективного дообучения модели BERT, поэтому необходимо было, чтобы неразмеченный набор данных был достаточно большим.
        \item Наличие вариативности в новостных источниках: наш инструмент в итоге должен получиться достаточно универсальным, так что с его помощью можно было бы размечать как, например, новости про спорт, так и новости про экологию.
        \item Возможность расширения: по возможности, способ сбора набора данных должен быть достаточно простым, чтобы можно было собрать самые новые статьи.
        \item Простота использования: наша работа нацелена более на аннотацию, чем на сбор данных, поэтому нужен набор данных, который будет легко использовать без дополнительной обработки.
    \end{itemize}
    
    Среди вариантов наборов данных, предложенных на сайте HuggingFace\footnote{https://huggingface.co/}, были рассмотрены следующие:
    \begin{itemize}
        \item news\_commentary\footnote{https://huggingface.co/datasets/news\_commentary} --- набор данных с переводом большого количества статей между различными языками. Не был выбран, так как данные изначально были собраны не для классификации, и понадобилась бы дополнительная обработка, чтобы вывести из него нужные статьи на английском языке.
        \item multi\_news\footnote{https://huggingface.co/datasets/multi\_news} --- набор данных для суммаризации, в котором для суммаризации даются статьи из разных источников. Не был выбран по той же причине: этот набор данных был собран для другой задачи, и была необходима предобработка, чтобы достать статьи в нужном нам формате.
        \item argilla\footnote{https://huggingface.co/datasets/argilla/news-summary} --- набор данных, содержащий статьи, изначально собранные для классификации. Содержит около 20000 статей, что подходит нам, но не был выбран, так как нет возможности собрать его заново из самых новых статей.
        \item cc\_news\footnote{https://huggingface.co/datasets/cc\_news} --- набор данных, состоящий из статей, собранных с помощью утилиты news-please\footnote{https://github.com/fhamborg/news-please}, которую легко использовать, и потенциально можно было бы использовать также для итогового инструмента командной строки. Этот набор данных прост в использовании, легко расширяется, а также содержит сотни тысяч статей, собранных из различных новостных порталов, поэтому он был выбран в качестве неразмеченного набора данных для нашей задачи.
    \end{itemize}
    
    \subsection{Подготовка исходного набора данных к аннотации}
    Выбранный набор данных содержал несколько сотен тысяч статей, что было слишком много для аннотации. Необходимо было отобрать столько самых подходящих статей, чтобы суммарно в них было около 20000 предложений. Также необходимо было разделить эти статьи на предложения, чтобы подготовить данные к разметке. Для этого были проделаны следующие шаги:
    \begin{itemize}
        \item Из набора данных были убраны все статьи, содержащие менее пяти предложений --- мы признали эти статьи выбросами из общего распределения.
        \item Статьи были дедуплицированы --- набор данных мог содержать одинаковые статьи из одного и того же источника, но разных веб-сайтов, например, reuters.co.uk и reuters.com.
        \item В оставшемся наборе данных были оставлены только источники, содержащие от 100 статей. Это было сделано для удаления слишком редких источников --- вероятных выбросов из общего распределения.
        \item Были выбраны самые новые статьи, так, чтобы суммарное число предложений было около 20000. В итоге было оставлено 1024 статьи из июля 2018 года, содержащие в сумме 19953 предложения. Мы посчитали, что июль 2018 года --- это достаточно новые статьи, и можно просто взять их, и таким образом, сбор новейших данных и разработка инструмента, использующего их, остаётся для будущей работы.
    \end{itemize}
    
    \subsection{Написание инструкции}
    Значительное количество времени было выделено на написание качественной инструкции. Для этого был предпринят следующий алгоритм действий. Изначально была составлена инструкция на основе \cite{annotating-private-states}, адаптированная под нашу конкретную задачу. Пользователям предлагалось выбрать либо опцию "неприменимо", предназначенную для предложений, не содержащих никакие утверждения и, следовательно, не подходящих для разметки, либо степень субъективности по дискретной шкале от 1 до 5, также известной как шкала Ликерта. Дальше происходили три итерации улучшения инструкции, состоящие в том, что автор и третье лицо размечали три заранее выбранных статьи и сравнивали результаты. После одной из итераций состоялась консультация с студентом Школы лингвистики НИУ ВШЭ, в ходе которой инструкция была улучшена. После трёх итераций удалось получить корелляцию 0.62 и F1-score 0.41 между авторскими ответами и ответами третьего лица. Так как особенность задачи состоит в субъективности разметки, такие результаты были признаны достаточно хорошими, чтобы запускать разметку полного набора данных.
    
    Получившаяся инструкция на английском языке доступна в Приложении А к данному документу.
    
    \subsection{Разметка с помощью краудсорсинга}
    Для более масштабной разметки был использован сервис Toloka AI\footnote{https://toloka.ai/}. В данном сервисе есть два важных понятия: проект и пул. Проект --- это сущность, содержащая несколько пулов. На уровне проекта задаётся инструкция и некоторые правила контроля качества. Пул --- это набор данных, который непосредственно размечают работники краудсорсинга. Бывают разные виды пулов, например, тренировочный пул создан для того, чтобы на примерах объяснить работникам инструкцию; этот пул размечается бесплатно. Экзаменационный пул необходим, чтобы отобрать работников с достаточным умением, и отсеять мошенников и роботов. Далее, существует общий вид пула, в котором работники уже непосредственно размечают рабочие данные. В нашем проекте были все эти три вида пулов. Для тренировочного пула было специально написано несколько примеров под каждую метку; также были написаны пояснения к этим примерам. Для экзаменационного пула были использованы те размеченные автором данные, которые использовались при улучшении инструкции. Остальные данные были размечены с помощью общего пула. Параметр перекрытия (то есть, сколько аннотаторов должны разметить одно предложение) был выбран равным 3. В качестве правил контроля качества использовались ограничения по числу размеченных наборов задач на человека за день, ограничение по пропуску наборов задач подряд, а также отложенная приёмка (то есть, перед оплатой денег разметка должна была быть проверена вручную, и аннотации могли быть отклонены). Изначально было размечено всего 5 статей, чтобы удостовериться в правильности настройки. После того, как настройки были улучшены, а разметка этих пяти статей выглядела адекватно, были запущены в разметку остальные статьи.
    
    \subsection{Пост-обработка разметки}
    Перед агрегацией ответов аннотаторов метки были отображены в более удобное пространство для модели: метки ``1'' и ``2'' отображаются в метку ``объективное'', метка ``3'' в метку ``нейтральное'', метки ``4'' и ``5'' в метку ``субъективное'', метка ``неприменимо'' в себя. Распределение меток доступно в таблице \ref{tab:label-distribution}.
    
    Затем для непосредственно агрегации был применён алгоритм WAWA\footnote{https://toloka.ai/docs/crowd-kit/reference/\\crowdkit.aggregation.classification.wawa.Wawa/}. Это итеративный алгоритм, который в начале выдаёт каждому работнику вес 1, потом вычисляет для каждого предложения голос взвешенного большинства и перераспределяет веса между работниками в зависимости от того, как близко они к этому большинству на каждом примере. После агрегации голосов аннотаторов для каждого работника был посчитан F1-score, и задачи, размеченные аннотаторами, получившими F1-score меньше 0.35, были переразмечены. Таким образом, нам удалось достичь достаточного уровня согласия между разными аннотаторами.
    
    Далее, данные были поделены на тренировочную, валидационную и тестирующую выборки в пропорции 0.72 : 0.08 : 0.2.
    
    \section{Обучение модели}
    В рамках данной статьи мы дообучили модель bert-base-cased\footnote{https://huggingface.co/bert-base-cased} на размеченном наборе данных. Мы поставили две задачи в рамках этого раздела: получить достаточно хорошее качество на тестирующей выборке --- то есть, сравнимое с качеством, полученным в статье \cite{bert-finetune} --- точность около 0.84, и протестировать успешные эксперименты из статьи \cite{bert-finetune}, а именно политику одного цикла и обучение с несколькими задачами.
    
    \subsection{Детали реализации и обучения}
    Обучение было реализовано на языке Python, с применением библиотеки PyTorch. В качестве оптимизатора использовался Adam. Базовая модель была взята с сайта HuggingFace, как и токенизатор. В качестве головы модели были взяты слой Dropout с параметром $p=0.1$ и линейный слой. Для каждого эксперимента в рамках оптимизации гиперпараметров мы обучали финальную модель на 5 эпох и брали модель с эпохи, на которой получилась лучшая точность на валидационной выборке. В рамках каждого эксперимента менялся какой-то один параметр, остальные фиксировались. Реализация доступна на сервисе GitHub\footnote{https://github.com/beastSL/hse-thesis/tree/main/model}.
    
    Обучение производилось на GPU NVidia A100 SXM4, GPU RAM 39.9GB, Total GPU TFlops 19.5. Машина была арендована на сервисе vast.ai.
    
    \subsection{Трансформация данных}
    Перед оптимизацией гиперпараметров было необходимо отобрать данные, которые мы бы подавали в модель. При выборе трансформации мы сначала обучили модель на нетрансформированных обучающих данных, вручную проанализировали её ошибки, и затем на основе этих ошибок попробовали трансформировать данные. В качестве экспериментов были рассмотрены следующие трансформации:
    \begin{itemize}
        \item Так как модель была сильно смещена в сторону объективных предложений и переобучалась на них, мы попробовали сбалансировать классы в обучающей выборке --- взяли случайную подвыборку объективных предложений, чтобы в итоге их было столько же, сколько субъективных.
        \item Так как модель почти никогда не предсказывала нейтральную метку, мы попробовали убрать из всех выборок примеры с нейтральной меткой. Данное решение в любом случае не повлияло бы на продуктовую составляющую: нам бы хотелось полностью размечать новостную статью на субъективность.
        \item Так как модель работала хуже из-за отсутствия контекста, мы попробовали добавить ко всем примерам предыдущее и следующее предложение из статьи. 
    \end{itemize}
    Результаты данной оптимизации доступны в разделе 6.

    \subsection{Выбор модели}
    Также необходимо было выбрать модель, которую мы будем дообучать. Изначально мы хотели дообучать BERT, поэтому выбор был лишь между bert-base-cased и bert-base-uncased\footnote{https://huggingface.co/bert-base-uncased}. Вторая модель использовалась в статье \cite{bert-finetune}, но наши данные содержат буквы в верхнем регистре. Результаты тестирования данных моделей приведены в разделе 6.

    \subsection{Изначальная оптимизация гиперпараметров}
    Перед тем, как применять продвинутые техники оптимизации, было необходимо оптимизировать базовые гиперпараметры. Мы оптимизировали скорость обучения, максимальное число токенов, которое можно пропустить в модель и параметр $\beta_2$ оптимизатора Adam. Кроме этого, мы попробовали заменить модель bert-base-uncased на bert-base-cased\footnote{https://huggingface.co/bert-base-cased}. Результаты данной оптимизации представлены в разделе 6.
    
    \subsection{Политика одного цикла}
    В качестве одной из продвинутых техник дообучения модели BERT было взято расписание скорости обучения, называемое ``политика одного цикла'', представленное в \cite{one-cycle-policy}. Согласно этому расписанию, cкорость обучения за все запланированные эпохи проходит через две фазы: нагревание (меньшая часть обучения, при которой скорость обучения поднимается с $\text{lr}_\text{init}$ до $\text{lr}_\text{max}$) и охлаждение (большая часть обучения, при которой скорость обучения спускается обратно). Опционально также есть третья фаза: аннигиляция, при которой скорость обучения в конце обучения опускается дальше до $\text{lr}_\text{min}$. Такое расписание придумано с целью предотвратить спуск к локальному высокому минимуму в начале обучения, затем ускорить обучение в зоне с хорошим градиентом (где-то посередине обучения), и затем уменьшать скорость обучения, чтобы не выйти из хорошего локального минимума. Также есть опция изменять импульс скорости обучения: можно также задать $\text{momentum}_\text{max}$ и $\text{momentum}_\text{min}$, но в этом случае импульс будет изменяться в обратную сторону: от максимального к минимальному и обратно (фазы аннигиляции для импульса не предполагается). Изменение импульса и самой скорости обучения показано на графике \ref{fig:one-cycle-policy}.
    \begin{figure}[h!]
        \centering
        \includegraphics*[width=\linewidth]{one-cycle-policy.jpg}
        \caption{}
        \label{fig:one-cycle-policy}
    \end{figure}
    
    Для выбора одного из наборов параметров, задающих скорость обучения при политике одного цикла, было применено ``тестирование диапазона скорости обучения'', описанное в \cite{lr-range-test}. Его суть заключается в следующем. Необходимо найти минимальную скорость обучения, при которой ошибка модели начинает расходиться. Эта скорость берётся за $\text{lr}_\text{max}$. Затем, $\text{lr}_\text{init}$ задаётся как $\frac{\text{lr}_\text{max}}{10}$, и $\text{lr}_\text{min} = \frac{\text{lr}_\text{max}}{100}$. Кроме этого набора параметров скорости обучения мы также пробовали оптимизировать другие (детали доступны в разделе 6).
    
    \subsection{Обучение с несколькими задачами}
    В качестве второй продвинутой техники дообучения было принято дообучение модели на нескольких задачах. Эта техника впервые была представлена в статье \cite{multi-task-learning}. Её суть заключается в том, что вместо того, чтобы дообучать модель и классификатор на одну задачу, мы будем обучаться на разные задачи, чередуя наборы данных из этих разных задач для оптимизации. Было показано, что данный подход успешно работает во многих приложениях, связанных с обработкой естественного языка и компьютерным зрением.
    
    \subsubsection{Наборы данных}
    В качестве задач мы взяли следующие наборы данных:
    \begin{itemize}
        \item Разумеется, мы взяли наш набор данных. Для удобства, здесь и далее мы будем обозначать его как NSDC (News Subjectivity Detection Corpus). На выходе будет ожидаться одна из трёх меток: неприменимо, объективное предложение, или субъективное предложение.
        \item Также мы взяли набор данных SUBJ из статьи \cite{subj}, в котором содержится 10000 предложений из обзоров фильмов, размеченных алгоритмически на субъективность (по 5000 на каждый класс). Детали разметки данного набора данных приведены в секции 2. Соответственно, на выходе ожидается одна из двух меток: объективность/субъективность.
        \item В качестве ещё одного набора данных в задаче обнаружения субъективности мы взяли набор данных Wikipedia biased statements из статьи \cite{wikipedia-biased-statements}. Данный набор данных был собран из предложений в Википедии, которые по мнению некоторых пользователей были отмечены содержащими чью-то субъективную точку зрения. Этот набор данных содержит 3109 объективно сформулированных предложений и 1843 субъективно сформулированных предложений. Детали разметки данного набора данных приведены в секции 2. Таким образом, это задача классификации отдельных предложений, ожидающая одну из двух меток на выходе.
        \item Также один из взятых наборов данных --- IMDb, представленный в статье \cite{imdb}. Он состоит из 50000 предложений из отзывов, взятых с сайта IMDb, размеченных на задачу анализа настроения (то есть, на задачу определения позитивности/негативности предложения). Данный набор данных также решает задачу классификации отдельных предложений, и ожидает одну из двух меток на выходе.
        \item Мы также взяли два набора данных для задачи классификации текстов. Один из них --- набор данных SNLI (Stanford Natural Language Inference), представленный в статье \cite{snli}. Он содержит 570152 пар предложений, вручную размеченных на соотношение второго предложения с первым, то есть возможны три метки: логическое следствие, нейтральность и противоречие.
        \item Второй набор данных для задачи классификации текстов --- QNLI (Stanford Question Answering dataset), представленный в статье \cite{qnli}. Он содержит 115669 пар вопрос-параграф, размеченный на задачу бинарной классификации: содержит ли параграф ответ на данный вопрос. В данном наборе данных вопросы были написаны аннотаторами вручную, а параграфы взяты из Википедии.
        \item Наконец, для задачи регрессии мы взяли набор данных STS-B (Semantic Textual Similarity Benchmark), представленный в статье \cite{sts-b}. Он содержит 8628 пар предложений из заголовков новостей, подписей к видео и картинкам, и других источников. Для каждой пары предложений необходимо оценить их близость по шкале от 0 до 5. 
    \end{itemize}
    
    Сводная информация по всем наборам данных представлена в таблице \ref{tab:datasets}.
    \begin{table}[h!]
        \begin{center}
            \begin{tabular}{|c|c|c|l|}
                \hline
                \thead{Набор \\ данных} & \thead{Число \\ примеров} & \thead[c]{Количество \\ меток} & \multicolumn{1}{c|}{\thead{Задача}} \\ \hline
                NSDC & 19953 & 4 & \makecell[l]{Обнаружение субъективности, \\ классификация предложений} \\ \hline
                SUBJ & 10000 & 2 &  \makecell[l]{Обнаружение субъективности, \\ классификация предложений} \\ \hline
                \makecell{Wikipedia \\ biased \\ statements} & 4952 & 2 & \makecell[l]{Обнаружение субъективности, \\ классификация предложений} \\ \hline
                IMDb & 50000 & 2 & \makecell[l]{Классификация предложений}  \\ \hline
                QNLI & 115669 & 2 & \makecell[l]{Классификация текстов} \\ \hline
                SNLI & 570152 & 3 & \makecell[l]{Классификация текстов} \\ \hline
                STS-B & 8628 & 1 & Регрессия \\ \hline
            \end{tabular}
            \caption{Сводная информация о данных}
            \label{tab:datasets}    
        \end{center}
    \end{table}
    \subsubsection{Детали реализации}
    Мы реализовали обучение с несколькими задачами следующим образом. Для каждого набора данных был введён отдельный линейный классификатор, при этом предобученная модель BERT оставалась общей для всех задач. Одна эпоха обучения состояла из поочерёдного обучения модели и классификатора на соответствующей задаче. При этом для оптимизации классификации использовался PyTorch модуль \texttt{CrossEntropyLoss}, а для оптимизации регрессии --- \texttt{MSELoss}. Визуально это представлено на рисунке \ref{fig:multi-task-learning-architecture}. После мультизадачного обучения модели на несколько эпох мы сохраняли модель и пробовали дообучить её на целевом наборе данных ещё на 5 эпох. В итоге сохранялась версия модели, дающая лучший результат на валидационной выборке.
    \begin{figure}[h!]
        \centering
        \includegraphics*[width=\linewidth]{multi-task-learning-architecture.jpg}
        \caption{Архитектура для обучения на несколько задач}
        \label{fig:multi-task-learning-architecture}
    \end{figure}
    
    Для оптимизации гиперпараметров мы пробовали перебирать множество задач, на которых обучалась модель и количество эпох, в течение которых модель обучалась на несколько задач. Результаты доступны в секции 6.

    \section{Создание СLI-инструмента}
    Для использования обученной модели мы создали CLI-инструмент. В рамках этой работы был реализован алгоритм, который по URL новостной статьи запускает локальный HTTP-сервер, на котором находится HTML-страница с размеченным на субъективность текстом новостной статьи. Данный алгоритм состоит из следующих шагов:
    \begin{enumerate}
        \item На вход алгоритму подаётся URL новостной статьи и директория, из которой будет запускаться HTTP-сервер.
        \item С помощью библиотеки news-please мы скачиваем новостную статью и сохраняем её заголовок, текст, разделённый на абзацы, и главную картинку.
        \item С помощью обученной модели мы размечаем весь текст на субъективность. Таким образом, каждому предложению сопоставляется либо метка ``неприменимо к разметке'', либо метка ``объективное'', либо метка ``субъективное''.
        \item Мы создаём текст HTML-документа с сохранёнными заголовком статьи, главной картинкой и размеченным текстом. Объективные предложения выделяются зелёным фоном, а субъективные --- красным. Мы сохраняем этот HTML-файл в указанную директорию, из которой пбудет запускаться сервер.
        \item С помощью команды \texttt{python3 -m http.server} мы запускаем локальный HTTP-сервер, содержащий сгенерированую веб-страницу.
    \end{enumerate}
    Также мы добавили опцию указывать вместо URL веб-страницы файл с текстом статьи. Такая альтернатива может быть полезна в некоторых применениях, например, если скачать текст статьи с помощью библиотеки news-please не представляется возможным. 
    
    Далее, мы опросили трёх человек об их опыте использования данного CLI-инструмента. Результаты доступны в секции 5.
    
    \section{Результаты}
    \subsection{Сбор данных}
    Мы собрали современный набор данных, размеченных на задачу обнаружения субъективности, с помощью разметки работниками краудсорсинга. Мы получили достаточно большой уровень согласия аннотаторов --- F1-score каждого аннотатора против агрегированных меток составляет не менее 0.35. Итоговое распределение меток доступно в таблице \ref{tab:label-distribution}.
    \begin{table}[h!]
        \begin{center}
            \begin{tabular}{|l|c|}
                \hline
                \multicolumn{1}{|c|}{\thead{Метка}} & \thead{Количество \\ предложений \\ с данной \\ меткой} \\ \hline
                Неприменимо & 1265 \\ \hline
                Объективное & 14255 \\ \hline
                Нейтральное & 516 \\ \hline
                Субъективное & 3917 \\ \hline
            \end{tabular}
            \caption{}
            \label{tab:label-distribution}    
        \end{center}
    \end{table}
    
    Результаты смещены в сторону объективных предложений, что ожидаемо, так как новости обычно пишутся в объективном стиле. Тем не менее, нам удалось собрать достаточно большое количество субъективных предложений, что достаточно для обучения модели, и поэтому сбор данных можно считать успешным.
    
    \subsection{Обучение модели}
    В данном разделе мы опишем результаты оптимизации гиперпараметров для обучения модели, а также вручную проанализируем типичные ошибки, которые она допускает.

    \subsubsection{Базовые модели}
    В качестве базовых моделей, от которых можно было бы отталкиваться при оценке модели, мы использовали классификатор, всегда выдающий самый популярный вариант, и логистическую регрессию, обученную на признаках TF-IDF. Результаты применения этих моделей доступны в таблице \ref{tab:baseline-results}.
    \begin{table}[h!]
        \begin{center}
            \begin{tabular}{|l|c|c|}
                \hline
                \multicolumn{1}{|c|}{\thead{Модель}} & \thead{Лучшая \\ точность на \\ валидационной \\ выборке} & \thead{Лучшая \\ F1-мера на \\ валидационной \\ выборке} \\ \hline
                \makecell[l]{Константный классификатор} & 0.7406 & 0.2837 \\ \hline
                \makecell[l]{TF-IDF + логистическая регрессия} & 0.774 & 0.4839 \\ \hline
            \end{tabular}
            \caption{Результаты выбора данных.}
            \label{tab:baseline-results}    
        \end{center}
    \end{table}

    \subsubsection{Трансформации данных}
    Для начала, мы протестировали различные трансформации данных для более эффективного дообучения модели. Мотивация, стоящая за выбором таких трансфромаций, изложена в секции 4. Результаты данной оптимизации доступны в таблице \ref{tab:data-optimization-results}. Так как отсутствие нейтральной метки заметно улучшило F1-меру, мы решили оставить эти данные. В следующих секциях все эксперименты проводились именно на таких данных.
    \begin{table}[h!]
        \begin{center}
            \begin{tabular}{|l|c|c|}
                \hline
                \multicolumn{1}{|c|}{\thead{Данные}} & \thead{Лучшая \\ точность на \\ валидационной \\ выборке} & \thead{Лучшая \\ F1-мера на \\ валидационной \\ выборке} \\ \hline
                \makecell[l]{Данные по умолчанию} & 0.8228 & 0.5733 \\ \hline
                \makecell[l]{Сбалансированная \\ обучающая выборка} & 0.7676 & 0.5737 \\ \hline
                \makecell[l]{Данные без \\ нейтральной метки} & \textbf{0.8255} & \textbf{0.7535}  \\ \hline
                \makecell[l]{Данные с контекстом} & 0.713 & 0.4071  \\ \hline
            \end{tabular}
            \caption{Результаты выбора данных.}
            \label{tab:data-optimization-results}    
        \end{center}
    \end{table}

    \subsubsection{Выбор модели}
    Также необходимо было выбрать модель, в рамказ чего мы протестировали модели bert-base-uncased и bert-base-cased с гиперпараметрами по умолчанию. Результаты тестирования доступны в таблице \ref{tab:model-optimization-results}. Как видно из результатов, мы решили выбрать модель bert-base-cased, так как выбор идёт в первую очередь по F1-мере.
    \begin{table}[h!]
        \begin{center}
            \begin{tabular}{|l|c|c|}
                \hline
                \multicolumn{1}{|c|}{\thead{Модель}} & \thead{Лучшая \\ точность на \\ валидационной \\ выборке} & \thead{Лучшая \\ F1-мера на \\ валидационной \\ выборке} \\ \hline
                bert-base-uncased & \textbf{0.8255} & 0.7535 \\ \hline
                bert-base-cased & 0.8213 & \textbf{0.7594} \\ \hline
            \end{tabular}
            \caption{Результаты выбора модели. Параметры, использованные при выборе: максимальное число токенов ($\text{max\_len}$ --- 512, скорость обучения ($\text{lr}$) --- 2e-5, параметр оптимизатора $\beta_2$ --- 0.99).}
            \label{tab:model-optimization-results}    
        \end{center}
    \end{table}

    \subsubsection{Изначальная оптимизация гиперпараметров}
    В качестве изначальной оптимизации гиперпараметров мы оптимизировали скорость обучения, максимальное число токенов, которое можно пропустить в модель и параметр $\beta_2$ оптимизатора Adam. В качестве сетки для скорости обучения были взяты значения $5 \cdot 10^{-6}$, $10^{-5}$, $2 \cdot 10^{-5}$ (значение по умолчанию), $5 \cdot 10^{-5}$ и $10^{-4}$. В качестве сетки для максимального числа токенов мы взяли 256 и 512 (значение по умолчанию). В качестве сетки для параметра $\beta_2$ мы взяли 0.98, 0.99 (значение по умолчанию) и 0.999. Результаты данной оптимизации доступны в таблице \ref{tab:initial-hyperparameters-optimization-results}.
    \begin{table}[h!]
        \begin{center}
            \begin{tabular}{|l|c|c|}
                \hline
                \multicolumn{1}{|c|}{\thead{Эксперимент}} & \thead{Лучшая \\ точность на \\ валидационной \\ выборке} & \thead{Лучшая \\ F1-мера на \\ валидационной \\ выборке} \\ \hline
                \makecell[l]{Гиперпараметры по умолчанию} & 0.8213 & 0.7594 \\ \hline
                $\text{max\_len} = 256$ & 0.8213 & 0.7594  \\ \hline
                $\text{lr} = 10^{-4}$ & 0.8115 & 0.737  \\ \hline
                $\text{lr} = 5 \cdot 10^{-5}$ & 0.8053 & 0.7644 \\ \hline
                $\text{lr} = 10^{-5}$ & \textbf{0.8255} & 0.767 \\ \hline
                $\text{lr} = 5 \cdot 10^{-6}$ & 0.8282 & 0.7639  \\ \hline
                $\beta_2 = 0.999$ & 0.8282 & 0.7771 \\ \hline
                $\beta_2 = 0.98$ & \textbf{0.8255} & \textbf{0.7798}  \\ \hline
            \end{tabular}
            \caption{Результаты изначальной оптимизации гиперпараметров. Значения по умолчанию такие: максимальное число токенов ($\text{max\_len}$ --- 512, скорость обучения ($\text{lr}$) --- 2e-5, параметр оптимизатора $\beta_2$ --- 0.99).}
            \label{tab:initial-hyperparameters-optimization-results}    
        \end{center}
    \end{table}

    Прокомментируем данные результаты. Мы смогли улучшить F1-меру на валидационной выборке на 0.02, присвоив параметру $\beta_2$ значение 0.98. Это не очень сильное улучшение, но от оптимизации таких параметров и не ожидался сильный прирост качества. При этом комбинирование разных экспериментов хороших результатов не дало.

    \subsubsection{Политика одного цикла}
    Здесь и далее будем считать, что за модель по умолчанию была принята модель, обученная на данных без нейтральной метки с гиперпараметрами оптимизатора по умолчанию, кроме $\beta_2$ (со значением 0.98). Мы реализовали политику одного цикла и попробовали оптимизировать её гиперпараметры. Мы оптимизировали следующие гиперпараметры: параметры скоростей обучения ($\text{lr}_\text{min}$, $\text{lr}_\text{max}$, $\text{lr}_\text{init}$), наборы импульсов ($\text{momentum}_\text{max}$ и $\text{momentum}_\text{min}$), пробовали дообучать с фазой аннигиляции.

    В библиотеке PyTorch $\text{lr}_\text{init}$ и $\text{lr}_\text{min}$ задаются через $\text{lr}_\text{max}$ с помощью делителей $\text{div\_factor}$ и $\text{final\_div\_factor}$ с помощью следующих уравнений:
    \begin{equation*}
        \left\{
        \begin{aligned}
            \text{lr}_\text{init} &= \frac{\text{lr}_\text{max}}{\text{div\_factor}}\\
            \text{lr}_\text{min} &= \frac{\text{lr}_\text{max}}{\text{final\_div\_factor}}\\
        \end{aligned}
        \right..
    \end{equation*}
    По умолчанию $\text{lr}_\text{max} = 2 \cdot 10^{-4}$, $\text{div\_factor} = 25$, $\text{div\_factor} = 10^4$, причём фаза аннигиляции отключена. С такими делителями мы перебирали $\text{lr}_\text{max}$ по сетке $[5 \cdot 10^{-6},\ 10^{-5},\ 2 \cdot 10^{-5},\ 5 \cdot 10^{-5},\ 10^{-4}]$. Отдельно мы пробовали $\text{div\_factor} = 10$ и включить фазу аннигиляции со стандартным $\text{final\_div\_factor}$. Также мы пробовали изменить параметры импульса. По умолчанию в PyTorch $\text{momentum}_\text{min} = 0.85$, $\text{momentum}_\text{max} = 0.95$, мы попробовали значения из статьи --- 0.8 и 0.9 cоответственно. Также мы попробовали скомбинировать параметры $\text{div\_factor} = 10$ и $\text{final\_div\_factor} = 100$, включив фазу аннигиляции. Результаты доступны в таблице \ref{tab:one-cycle-policy-optimization-results}.
    \begin{table}[h!]
        \begin{center}
            \begin{tabular}{|l|c|c|}
                \hline
                \multicolumn{1}{|c|}{\thead{Эксперимент}} & \thead{Лучшая \\ точность на \\ валидационной \\ выборке} & \thead{Лучшая \\ F1-мера на \\ валидационной \\ выборке} \\ \hline
                Без политики одного цикла & 0.8255 & \textbf{0.7798} \\ \hline
                Гиперпараметры по умолчанию & 0.8157 & 0.7605 \\ \hline
                $\text{lr}_\text{max} = 5e-5$ & \textbf{0.8268} & 0.772 \\ \hline
                $\text{lr}_\text{max} = 2e-5$ & \textbf{0.8268} & 0.7637 \\ \hline
                $\text{lr}_\text{max} = 1e-5$ & 0.8227 & 0.7542 \\ \hline
                $\text{lr}_\text{max} = 5e-6$ & 0.8157 & 0.7508 \\ \hline
                $\text{div\_factor} = 10$ & 0.8178 & 0.7515 \\ \hline
                \makecell[l]{$\text{momentum}_\text{min} = 0.8$ \\ $\text{momentum}_\text{max} = 0.9$} & 0.822 & 0.7527 \\ \hline
                С фазой аннигиляции & 0.8206 & 0.7667 \\ \hline
                \makecell[l]{$\text{div\_factor} = 10$ \\ $\text{final\_div\_factor} = 10$ \\
                С фазой аннигиляции} & 0.8178 & 0.7564 \\ \hline
            \end{tabular}
            \caption{Результаты оптимизации гиперпараметров политики одного цикла. Значения по умолчанию такие: фаза аннигиляции отключена, $\text{lr}_\text{max} = 10^{-4}$, $\text{div\_factor} = 25$, $\text{final\_div\_factor} = 10^4$, $\text{momentum}_\text{min} = 0.85$, $\text{momentum}_\text{max} = 0.95$.}
            \label{tab:one-cycle-policy-optimization-results}    
        \end{center}
    \end{table}
    
    Комментируя эти результаты, можно подытожить, что политика одного цикла не внесла положительного эффекта в обучение модели.
    \subsubsection{Обучение с несколькими задачами}
    Вторая продвинутая техника, которую мы реализовали~--- обучение с несколькими задачами. Здесь гиперпараметра всего два~--- множество задач, на которых мы обучаем модель перед финальным дообучением на целевом наборе данных, и количество эпох мультизадачного обучения. Мы пробовали дообучать модель на целевом наборе данных (NSDC) после любого количества эпох, после которых ошибка модели на валидационном наборе данных продолжала падать. Множества задач были взяты следующие: 
    \begin{itemize}
        \item Множество задач определения субъективности --- NSDC, SUBJ, Wikipedia biased statements. Здесь пробовали дообучать после одной и двух мультизадачных эпох.
        \item Множество задач классификации одного предложения --- NSDC, SUBJ, Wikipedia biased statements, IMDb. Здесь пробовали обучать после одной мультизадачной эпохи.
        \item Всё множество наборов данных, описанных в секции 4. Здесь пробовали обучать после одной и двух мультизадачных эпох.
    \end{itemize}
    При выборе числа эпох для такого ``предобучения'' мы руководствовались ошибкой модели на целевом наборе данных после каждого прохода через все наборы данных. Если ошибка модели продолжала идти вниз после какой-то эпохи, то позже мы пробовали дообучить модель, начиная с сохранения модели, сделанного в конце этой эпохи. Результаты оптимизации гиперпараметров доступны в таблице \ref{tab:multi-task-learning-optimization-results}.
    \begin{table}[h!]
        \begin{center}
            \begin{tabular}{|l|c|c|}
                \hline
                \multicolumn{1}{|c|}{\thead{Эксперимент}} & \thead{Лучшая \\ точность на \\ валидационной \\ выборке} & \thead{Лучшая \\ F1-мера на \\ валидационной \\ выборке} \\ \hline
                \makecell[l]{Без обучения с \\ несколькими задачами} & \textbf{0.8255} & \textbf{0.7798} \\ \hline
                I, после 1 эпохи & 0.8248 & 0.7471 \\ \hline
                I, после 2 эпох & 0.8317 & 0.7563 \\ \hline
                II, после 1 эпохи & 0.8303 & 0.7558 \\ \hline
                III, после 1 эпохи & 0.8317 & 0.7601 \\ \hline
                III, после 2 эпох & 0.8303 & 0.7572 \\ \hline
            \end{tabular}
            \caption{Результаты оптимизации гиперпараметров обучения с несколькими задачами. I, II и III --- множества наборов данных, в порядке, описанном выше.}
            \label{tab:multi-task-learning-optimization-results}    
        \end{center}
    \end{table}
    
    Комментируя эти результаты, можно подытожить, что обучение с несколькими задачами не внесло положительного эффекта в обучение модели.
    
    \subsubsection{Обсуждение и анализ ошибок модели}
    Лучший полученный результат --- F1-мера 0.7798 и точность 0.8255. Этот результат сопоставим с результатом, полученным в статье \cite{bert-finetune} на наборе данных Wikipedia biased statements, равным 0.84. Также можно упомянуть, что данная F1-мера намного лучше базовых моделей --- логистическая регрессия, обученная поверх признаков TF-IDF даёт F1-меру всего около 0.48. Поэтому в целом можно считать, что обучение модели прошло успешно.

    Тем не менее, хотелось бы провести качественный анализ ошибок модели, чтобы понять направления дальнейшего улучшения. Мы просмотрели 100 примеров из тестирующей выборки, на которых модель отвечала неправильно. В этом разделе мы обсудим основные свойства этих ошибок и возможные направления для улучшения модели.
    
    Для начала, для каждого из 100 примеров мы проверили, действительно ли модель не права. Оказалось, что модель действительно ошибается лишь в 51\% случаев. Это неудивительно, так как задача обнаружения субъективности сама по себе крайне субъективна. Во многих случаях сложно точно количественно оценить, насколько предложение субъективно, особенно не имея контекста.
    
    Из тех предложений, в которых модель ошибалась, особенно заметны следующие тенденции:
    \begin{itemize}
        \item Модель не воспринимает контекст. На вход модели всегда подаётся лишь одно предложение, но, как мы упоминали раньше, контекст достаточно важен при выявлении субъективности. Например, в случае прямой речи на несколько предложений, модель не будет знать о том, что предложения, находящиеся посередине этой прямой речи, принадлежат ей, и поэтому будет трактовать их неверно. Данная проблема отражена в 12 из 51 ошибочного примера. Вероятно, модель можно улучшить, подав на вход вдобавок к текущему предыдущее и следующее предложения, но данный эксперимент не дал нам прироста к качеству.
        \item Модель хуже аннотаторов понимает, когда выражается мнение третьего лица. В инструкции есть параграф про то, что предложения вида ``Он сказал, что ...'' считаются объективными; в целом, предложения такого вида следует судить по словам, которые указывают на прямую речь, такие как ``сказать'' или ``осудить''. Данная проблема отражена в 8 из 51 ошибочного примера. Вероятно, эти ошибки можно было бы исправить, классифицировав предложения на те, которые говорят о мнениях человека в первом и третьем лице.
        \item Остальные ошибки модели не поддаются какой-то общей тенденции, но можно сказать, что почти все они вызваны неверным пониманием семантики предложения. 
    \end{itemize}
    В таблице \ref{tab:error-analysis} приведены примеры, иллюстрирующие вышеописанные тенденции.
    \begin{table}[h!]
        \begin{center}
            \begin{tabular}{|l|c|c|l|}
                \hline
                \thead{Предложение} & \thead{Аннотаторская \\ метка} & \thead{Метка \\ модели} & \thead{Пояснение} \\ \hline
                \makecell[l]{"At the end it is \\ the American \\ consumer who \\ will pay the price \\ for Mr Trump's \\ policy.} & 1 & 3 & \makecell[l]{В оригинальном тексте \\ указателем на эту прямую \\ речь было слово said, \\ поэтому правильная \\ метка --- объективное \\ предложение. Но этот \\ контекст не подаётся в \\ модель, поэтому она \\ ошибается.} \\ \hline
                \makecell[l]{"We have spoken \\ a lot about \\ togetherness," \\ Kane said, "and \\ we've got a great \\ bond off the \\ pitch."} & 1 & 3 & \makecell[l]{Слово said в этом \\ предложении маркирует \\  прямую речь, и так как \\ оно нейтральное, \\ предложение \\ сформулировано \\ объективно. Модель \\ периодически не \\ справляется с \\ пониманием этого \\ правила.} \\ \hline
                \makecell[l]{Perhaps Amazon \\ is earning all \\ its government \\ contracts on a \\ level playing \\ field.} & 3 & 1 & \makecell[l]{В некоторых случаях \\ модель попросту  не \\ справляется \\ семантически понять, что \\ предложение \\ субъективное.} \\ \hline
            \end{tabular}
            \caption{Примеры, иллюстрирующие некоторые тенденции в ошибках модели и аннотаторов. Напомним, что метки значат следующее: 0 --- неприменимо к разметке, 1 --- объективное предложение, 3 --- субъективное предложение.}
            \label{tab:error-analysis}    
        \end{center}
    \end{table}
    \subsection{СLI-инструмент}
    Для использования проделанной нами работы мы создали CLI-инструмент, работающий с новостными статьями по URL. Необходимо упомянуть, что так как текст новостей собирался с помощью библиотеки news-please, для некоторых новостных порталов (например, BBC) данный инструмент не работает. Для обработки таких сервисов был написан режим работы инструмента, при котором на вход подаётся файл с самим текстом статьи. Тем не менее, многие известные издания, такие, как The Guardian, Daily Mail, Reuters и многие другие поддерживаются данной библиотекой. 
    
    После создания CLI-инструмента мы опросили трёх человек об их опыте использования данного инструмента. Их отзывы доступны в таблице \ref{tab:cli-tool-reviews}. Комментируя эти отзывы, можно подытожить, что инструмент действительно полезен при прочтении новостных статей, а негативная часть этих отзывов совпадает с тенденциями, которые мы заметили при анализе ошибок модели.
    \begin{table}[h!]
        \begin{center}
            \begin{tabular}{|l|}
                \hline
                \multicolumn{1}{|c|}{\thead{Отзыв}} \\ \hline
                Так, ну, я почитал, вчитываясь в саму новость, цвета помогают не \\ забывать, что именно ты читаешь --— сообщение о факте или чьё-то \\ чужое мнение про факт. У меня сложилось ощущение, что иногда оно \\ похожие цитаты разным цветом красит, но вероятно, показалось. Идея \\ крутая. \\ \hline
                На глаз видно, что он не совсем точно разделяет субъективные и \\ объективные штуки, как будто некоторым предложениям не хватает \\ контекста от предыдущих и от этого разделение странное получается. \\ При этом текст, помеченный субъективным, читать обычно интереснее --- \\ вероятно, потому что в них содержатся мнения. \\ \hline
                При прочтении статьи с выделенными субъективными предложениями \\ у меня сложилось ощущение, что этим предложениям доверять нельзя. \\ Особенно если это цитаты или вставки из диалогов. Это помогает не \\ верить на 100\% информации из этих предложений, а наоборот, \\ задумываться, насколько вообще эта мысль верна. Помогает сложить свое \\ мнение на этот счет. \\ \hline
            \end{tabular}
            \caption{Отзывы об опыте использования CLI-инструмента.}
            \label{tab:cli-tool-reviews}    
        \end{center}
    \end{table}

    \section{Заключение}
    В этой статье описана работа по созданию нового инструмента для распознавания мнений и фактов в новостных статьях. Был собран набор данных, необходимый для эффективного дообучения модели, проведено само дообучение, а также создан CLI-инструмент для применения данной модели. Мы уверены, что этот проект значительно повлияет на анализ новостей, и инструмент будет достаточно удобным для публичного распространения. Также мы считаем, что данная работа, в частности, создание такого набора данных, окажет положительное влияние на развитие сферы обнаружения субъективности.
    
\newpage 

\bibliographystyle{IEEEtran}
\bibliography{refs}
	
\newpage
\appendix
\section{Инструкция для аннотаторской разметки}
Мы прикладываем неотредактированную инструкцию на английском языке, которая была непосредственно доступна аннотаторам при разметке данных.

\section*{Annotation instructions}
This document describes the annotation instructions for subjectivity detection in news articles. First, we will describe what should be treated as subjectivity and objectivity and how to spot it. Then, we will fully describe the annotation task.

\subsection*{Subjectivity and objectivity}
First, we would like to note there are no formal definitions of subjectivity and objectivity. In many cases, you will have to appeal to your intuition and your reaction after reading a sentence. However, we will try to advise on how to notice subjectivity and help build up the needed intuition.

\subsubsection*{Types of subjectivity}
Subjectivity is an expression that represents opinions, beliefs, thoughts, feelings, emotions, goals, evaluations, and/or judgments (we'll use the expression "private states" to generalize these concepts). One key sign of it is that it is not open to objective observation or verification. Subjective expressions are the ones that cannot be refuted or confirmed.

The main types of subjectivity are:
\begin{itemize}
    \item direct mentions of the private state. This is the easiest-to-spot sign of subjectivity, including words that directly indicate private states. Examples: \begin{itemize}
        \item The U.S. fears a spill-over.
        \item I really enjoyed the book I read last night.
    \end{itemize}
    These sentences are highly subjective because the words "fear" directly indicate the emotion of fear, and the word "enjoyed" directly indicates the emotion of joy.
    \item expressive subjective elements. These are words or expressions in the text that implicitly indicate subjectivity. Here are some examples:
    \begin{itemize}
        \item The report is full of absurdities.
        \item The sunset painted the sky with a fiery orange hue.
    \end{itemize}
    We think these sentences are subjective because the expressions "full of absurdities" and "fiery" represent the writer's opinions or emotions.
\end{itemize}
Note that this is not an exhaustive list of subjectivity types. In some cases, you'll need to apply your intuition and appeal to the reaction a sentence gives you.

\subsubsection*{Subjectivity when describing private states of a third party}
A common case is sentences describing the private states (opinions, emotions, etc.) of a third party. One good example of such a description is direct or indirect speech (see the first two examples). Here are some examples of such sentences:
\begin{itemize}
    \item Sargeant O’Leary said the incident took place at 2:00 pm.
    \item Defence officials accused Beijing of using President Tsai's US visit as an "excuse to conduct military exercises".
    \item These people remember the horrors of World War II.
\end{itemize}
When annotating such sentences, you should not base your judgment on the private states of the third party themselves (e.g. "the incident took place at 2:00pm"). Instead, you should base your judgments on how these descriptions are presented in the sentence, and whether a certain tone is given to the private states. For example, words like "say", "know", and "want" are neutral, while words like "fear" and "accuse" give an intonation to the private states. Note that this way we are incentivizing you to spot specifically the writer's subjectivity.

We think sentences 1 and 3 are objective, because the words "said" and "remember" sound neutral, and the second sentence is subjective, because the word "accused" does indicate a tone of accusation.

\subsubsection*{Objective sentences}
The sentences that do not contain any subjectivity above and that present statements are considered objective. Note that these statements are not necessarily correct. Some examples:
\begin{itemize}
    \item The Earth is flat.
    \item The Dow Jones Industrial Average closed at 34,035.99 points on Monday.
\end{itemize}
\subsubsection*{Other important advice}
The other important things we need to mention before describing the exact task:
\begin{itemize}
    \item There are no fixed rules about how particular words should be annotated. The instructions describe the annotations of specific examples but do not state that specific words should always be annotated a certain way.
    \item Sentences should be interpreted with respect to the contexts in which they appear. You should not take sentences out of context and think about what they could mean but rather should judge them as they are being used in that particular sentence and document.
    \item It is impossible to cover all types of sentences in this instruction. For example, there could be sentences containing both objective and subjective elements. The subjective elements can also play a minor role in the sentence. You should base your judgment on your inner reaction and intuition after reading a sentence.
\end{itemize}
\subsection*{Task}
You will be consequently given sentences from a newspaper. Every sentence will be surrounded by several adjacent sentences to provide context, but the current sentence you're labelling will be highlighted.

Before labelling the main pool, you will need to pass training and an exam. Note that in order to get paid you will need to get 35\% correct responses on the exam. Also note that if your responses are on average too far from the majority vote on the main pool, your responses will be looked through and can be rejected.

Please note that some sentences might contain explicit language since the papers were scraped from the Internet.

Your task is to assign each sentence a subjectivity score. The score will be measured on a discrete scale from 1 to 5. You will also be given the option to assign a "Not applicable" label. Here are the explanations of the scale:

\begin{itemize}
    \item The "Not applicable" label is used when a sentence does not contain any statements, and therefore it is impossible to say if it is subjective or objective. Some cases for the "Not Applicable" label are incomplete sentences, questions and sentences fully consisting of noise. (see below for examples)
    \item Score 1 should be assigned when you are confident that the sentence is objective.
    \item Score 2 should be assigned if you are unconfident but suspect that the sentence is objective.
    \item Score 3 should be assigned if a sentence presents a statement but it is difficult to say whether the sentence is objective or subjective. Note that this option corresponds to the case when neither the instruction nor the intuition can help to decide if the sentence is subjective or objective, even though the annotation applies.
    \item Score 4 should be assigned if you are unconfident but suspect that the sentence is subjective.
    \item Score 5 should be assigned when you are confident that the sentence is subjective.
\end{itemize}
\end{document}